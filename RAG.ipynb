{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceInferenceAPIEmbeddings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document, GPTVectorStoreIndex, ServiceContext\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load PDF document and split into chunks\n",
    "loader = PyPDFLoader(\"SWE-Bench.pdf\")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load HuggingFace BGE Embedding Model\n",
    "model_name = \"BAAI/bge-large-en\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Initialize Qdrant Client for the vector store\n",
    "qdrant_client = QdrantClient(\n",
    "    url=\"https://c99d7921-24d0-4759-8836-938fa2f15d91.europe-west3-0.gcp.cloud.qdrant.io:6333\", \n",
    "    api_key=\"6cz2PvQQIfqLu2ALzGiIOCUSMBEaLV2W4MudoxYXOfywU4kt3Mu6Cw\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Convert text chunks into LlamaIndex Document objects\n",
    "llama_documents = [Document(text=doc.page_content) for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Convert the texts into nodes for LlamaIndex (equivalent of documents in LangChain)\n",
    "node_parser = SimpleNodeParser()\n",
    "nodes = node_parser.get_nodes_from_documents(llama_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Initialize the vector store with Qdrant and the HuggingFace embeddings\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=qdrant_client,\n",
    "    collection_name=\"vector_db\",  # The collection name in Qdrant\n",
    "    embedding_model=embeddings  # Use the HuggingFace BGE embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Build the LlamaIndex with the Qdrant vector store\n",
    "index = GPTVectorStoreIndex(\n",
    "    nodes=nodes,\n",
    "    vector_store=vector_store,\n",
    "    embed_model=embeddings  # Explicitly pass the HuggingFace embeddings model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: SWE-bench dataset contains how many number of problem examples?\n",
      "\n",
      "Query: How does SWE-bench evaluate an LLM?\n",
      "Answer: SWE-bench evaluates an LLM by generating a set of problem examples and comparing the generated output to the ground truth. The evaluation metric is the F1 score, which measures the proportion of correct predictions.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Define the HuggingFaceLLM class\n",
    "class HuggingFaceLLM:\n",
    "    def __init__(self, model_name: str):\n",
    "        # Load the tokenizer and model from Hugging Face\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        \n",
    "    def complete(self, prompt: str):\n",
    "        # Tokenize input prompt\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        \n",
    "        # Generate output using the model\n",
    "        outputs = self.model.generate(**inputs, max_length=500, num_return_sequences=1)\n",
    "        \n",
    "        # Decode the generated tokens to get the response\n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        return response\n",
    "\n",
    "# Instantiate the Hugging Face LLM\n",
    "llm = HuggingFaceLLM(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "\n",
    "# Custom query engine logic (depends on your existing setup)\n",
    "# Assuming `index` is already built and configured\n",
    "class CustomQueryEngine:\n",
    "    def __init__(self, index, llm):\n",
    "        self.index = index\n",
    "        self.llm = llm\n",
    "\n",
    "    def query(self, query_str):\n",
    "        # Retrieve relevant context (mocking retrieval from index for demonstration)\n",
    "        context_str = \"\"\n",
    "        \n",
    "        # Use the LLM to complete the query based on the context\n",
    "        prompt = f\"Context: {context_str}\\n\\nQuery: {query_str}\\nAnswer:\"\n",
    "        response = self.llm.complete(prompt)\n",
    "        return response\n",
    "\n",
    "# Instantiate the query engine with the Hugging Face LLM\n",
    "query_engine = CustomQueryEngine(index, llm)\n",
    "\n",
    "# Query the engine\n",
    "response = query_engine.query(\"SWE-bench dataset contains how many number of problem examples?\")\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
